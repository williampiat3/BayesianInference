{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Inference\n",
    "in this jupyter notebook our goal is to present the basic of bayesian inference and to see how we can apply it to different situations. This notebook contains a small introduction to bayesian inference, however I still have a lot to learn in the field so this presentation is also, for me, to make sure that I understood the principles\n",
    "\n",
    "## Bayesian statistics \n",
    "Bayesian statiscs aims at computing the probability of differents parameters at the light of some observation: we mesure the impact of observed data on our assumptions concerning these parameters\n",
    "\n",
    "Bayesian statistics are relying mostly on the Bayes formula:\n",
    "$$ \n",
    "P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "This formula means that observing the effect of A on B can give us some information on the probability of A if we know the possibilities of A and B.\n",
    "\n",
    "If you are confused (I was definitly confused at this moment) here is an example from wikipedia (french page) that I suggest we resolve:\n",
    "\n",
    "### Laplace: sex ratio in population\n",
    "\n",
    "Laplace observe in year 1785 a discrepancy between the birth of boys and girls, respectively $N_1 = 251 527$ and $N_2=241 945$ . He want to determine if the this difference is showing some biased probability $\\theta$ of having a boy.\n",
    "\n",
    "Assuming nothing on $\\theta$ (ie $\\theta$ follows a uniform law between 0 and 1) he want to compute\n",
    "\n",
    "$$\n",
    "P(\\theta \\leq \\frac{1}{2}|(N_1,N_2))\n",
    "$$\n",
    "\n",
    "Using Bayes formula:\n",
    "$$\n",
    "P(\\theta|(N_1,N_2))=\\frac{P((N_1,N_2)|\\theta)P(\\theta)}{P((N_1,N_2))}\n",
    "$$\n",
    "\n",
    "Ok so the easiest term in the previous equation is $P(\\theta)$ as we assumed $\\theta$ to follow a uniform law between 0 and 1 therefore $P(\\theta)=\\frac{1}{1-0}=1$\n",
    "\n",
    "$P((N_1,N_2)|\\theta)$ is $N_1$ draws with probability $\\theta$  and $N_2$ draws with probability $(1-\\theta)$ therefore:\n",
    "$$\n",
    "P((N_1,N_2)|\\theta)=\\theta^{N_1}(1-\\theta)^{N_2}\n",
    "$$\n",
    "\n",
    "$P((N_1,N_2))$ is equal to $P((N_1,N_2)|\\theta)$ summed over all possible values of $\\theta$:\n",
    "\n",
    "$$\n",
    "P((N_1,N_2))=\\int_0^1 P((N_1,N_2)|\\theta)d\\theta = \\int_0^1 \\theta^{N_1}(1-\\theta)^{N_2} d\\theta = B(N_1+1,N_2+1)\n",
    "$$\n",
    "\n",
    "With B the [beta function](https://en.wikipedia.org/wiki/Beta_function)\n",
    "\n",
    "In the end we have:\n",
    "\n",
    "$$\n",
    "P(\\theta|(N_1,N_2))=\\frac{\\theta^{N_1}(1-\\theta)^{N_2}}{B(N_1+1,N_2+1)}\n",
    "$$\n",
    "\n",
    "by definition:\n",
    "\n",
    "$$\n",
    "P(\\theta \\leq \\frac{1}{2}|(N_1,N_2))= \\int_0^{\\frac{1}{2}} P(\\theta|(N_1,N_2))d\\theta =  \\frac{\\int_0^{\\frac{1}{2}}\\theta^{N_1}(1-\\theta)^{N_2}d\\theta}{B(N_1+1,N_2+1)}= I_{\\frac{1}{2}}(N_1+1,N_2+1)\n",
    "$$\n",
    "\n",
    "with $I_x(a,b)$ being the [regularized incomplete beta function](https://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function)\n",
    "\n",
    "Lucky for us this regularized incomplete beta function is implemented in scipy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1460584897220391e-42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function betainc here is not the incomplete beta function but the regularized incomplete beta function\n",
    "# check out the docs of scipy: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.special.betainc.html\n",
    "# it is normalised using the beta function in the form of a gamma function ratio\n",
    "from scipy.special import betainc\n",
    "N1 = 251527\n",
    "N2 = 241945\n",
    "x = 0.5\n",
    "betainc(N1+1,N2+1,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of having a proportion of birth of boys inferior to 0.5 is equal to $1.15 \\times 10^{-42}$ in the light of such observation. We can call it unlikely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference\n",
    "In Machine learning we are often facing the problem of finding the \"best\" set of parameters of a parametric function to minimize the Mean Square Error however this set of parameter can not be the optimum that is the most robust. Let $\\theta \\in \\mathbb R^N$ the vector of parameters that we would like to find, $(X,Y)=D$ our data.\n",
    "We would like to have the following quantity:\n",
    "$$\n",
    "P(\\theta|D)\n",
    "$$\n",
    "Meaning the probability of our parameters once we observed the data.\n",
    "\n",
    "\n",
    "Using Bayes formula:\n",
    "$$\n",
    "P(\\theta|D)=\\frac{P(D|\\theta)P(\\theta)}{P(D)}\n",
    "$$\n",
    "\n",
    "$P(D|\\theta)$ is the likelyhood of the data, $P(\\theta)$ is our prior belief of the values of the parameters and $P(D)$ is the likelyhood summed over all the parameters\n",
    "\n",
    "Of course if $N=2$ the integral denominator is quite easy, however the integral becomes much more dificult in the case of a higly dimentional $\\theta$ however the denominator is a constant of theta therefore we can write:\n",
    "\n",
    "$$\n",
    "P(\\theta|D) \\propto P(D|\\theta)P(\\theta)\n",
    "$$\n",
    "\n",
    "And you'll see: that's just about what we need!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Hastings algorithm\n",
    "The [Metropolis Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) Monte Carlo Markov Chain method to sample from a probability distribution from which direct sampling is difficult.\n",
    "Let $P : \\mathbb R^n \\rightarrow \\mathbb R$ be the distribution that we want to sample however we don't have acces to $P$ but a function $f : \\mathbb R^n \\rightarrow \\mathbb R$ that is proportionate to $P$: \n",
    "$$\n",
    "P(x) \\propto f(x)\n",
    "$$\n",
    "We need also a transition distribution $g:\\mathbb R^n \\times \\mathbb R^n \\rightarrow \\mathbb R$ that tells us how we can pass from a postion $x$ to a position $x'$, a classical choice of $g$ is a gaussian perturbation around $x$. We will consider the case of $g$ being symetric it will simplify the steps of the algorithm.\n",
    "\n",
    "The algorithm works this way:\n",
    "* Initialisation: initialize $x_0 \\in \\mathbb R^n $\n",
    "* Sample $x' \\sim g(.|x)$ \n",
    "* Compute $ \\alpha = \\frac{P(x')}{P(x_t)}= \\frac{f(x')}{f(x_t)}$\n",
    "* with probability $p = min(1,\\alpha)$: $x_{t+1} \\leftarrow x'$ otherwise $x_{t+1}=x_t$\n",
    "\n",
    "After a certain number of step $x_t$ follows the probability distribution $P$\n",
    "\n",
    "The number of steps depends on the dimension $n$, of the function $g$. THere are lots of heuristics that allow to know if the MCMC converged.\n",
    "\n",
    "We will take simple examples, the computation won't take long and we will perform a lot of iterations to make sure that we converged to the distribution $P$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple case where we sample a sum of two gaussian distribution\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sum of two gaussian distributions\n",
    "def pdf(x):\n",
    "    return  1/math.sqrt(2*math.pi)*math.exp(-0.5*(x-2)**2) + 1/math.sqrt(2*math.pi)*math.exp(-0.5*(x+1)**2)\n",
    "\n",
    "# transition function\n",
    "def step_dist(x,sigma=0.2):\n",
    "    return x+ sigma*np.random.randn(1)\n",
    "\n",
    "# metropolis hasting algorithm\n",
    "def metropolis_hastings(number_of_steps,x0,pdf):\n",
    "    x=x0\n",
    "    for step in range(number_of_steps):\n",
    "        #sampling new state\n",
    "        x_test =  step_dist(x)\n",
    "        #computing alpha\n",
    "        alpha = pdf(x_test)/pdf(x)\n",
    "        #if ap\n",
    "        if alpha >=1:\n",
    "            x = x_test\n",
    "        else:\n",
    "            if  random.random() < alpha:\n",
    "                x = x_test\n",
    "    return x[0]\n",
    "\n",
    "def run_mcmc():\n",
    "    number_of_steps=2000\n",
    "    number_of_sample=1000\n",
    "    #initialisation at 0.5\n",
    "    x0 = 0.5\n",
    "    #running the algorithm  multiple times\n",
    "    results = [metropolis_hastings(number_of_steps,x0,pdf) for _ in tqdm(range(number_of_sample))]\n",
    "    plt.hist(results,bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:43<00:00,  9.57it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_mcmc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Bayesian inference\n",
    "We will apply the metropolis algorithm to compute the posterior distribition:\n",
    "\n",
    "Remember that we had the following relation:\n",
    "$$\n",
    "P(\\theta|D) \\propto P(D|\\theta)P(\\theta)\n",
    "$$\n",
    "This time we will apply the algorithm to $\\theta$, the function $f(\\theta)=P(D|\\theta)P(\\theta)$\n",
    "\n",
    "We can compute $f$ as $P(D|\\theta)$ is the likelihood of the data , $P(\\theta)$  is the prior that we gave to our parameters\n",
    "\n",
    "The likelihood depends on the problem that we consider: for regression or classification the computation is different, I will present here the case of regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "class linearModel():\n",
    "    def __init__(self,mu,sigma,dim):\n",
    "        self.mu = np.ones(dim)*mu\n",
    "        self.sigma = np.ones(dim)*sigma\n",
    "        self.mu_bias = mu\n",
    "        self.sigma_bias = sigma\n",
    "        self.dim = dim\n",
    "    def forward(self,X,w_sampled,bias_sampled):\n",
    "        return np.dot(X,w_sampled)+bias_sampled\n",
    "\n",
    "    #probability of the weights following a normal prior\n",
    "    def proba_weights(self,w_sampled,bias_sampled):\n",
    "        prob_w = np.prod((1/math.sqrt(2*math.pi)/self.sigma)*np.exp(-.5*(w_sampled-self.mu)**2/self.sigma))\n",
    "        prob_b = (1/math.sqrt(2*math.pi)/self.sigma_bias)*np.exp(-.5*(bias_sampled-self.mu_bias)**2/self.sigma_bias)\n",
    "        return prob_b*prob_w\n",
    "    #probability of Y knowing the weights and biases\n",
    "    def proba_y(self,X,w_sampled,bias_sampled,Y,noise):\n",
    "        y_hat = self.forward(X,w_sampled,bias_sampled)\n",
    "        proba_y = np.prod((1/math.sqrt(2*math.pi)/noise)*np.exp(-.5*(y_hat-Y)**2/noise))\n",
    "        return proba_y\n",
    "\n",
    "    #probability of the weights knowing the input data\n",
    "    def bayesian_proba(self,X,w_sampled,bias_sampled,Y,noise):\n",
    "        return self.proba_weights(w_sampled,bias_sampled)*self.proba_y(X,w_sampled,bias_sampled,Y,noise)\n",
    "\n",
    "#step function that we picked to be symetric\n",
    "def step_dist(sigma=0.15,shape=1):\n",
    "    return sigma*np.random.randn(shape)\n",
    "\n",
    "#Sampling the posterior using metropolis hastings\n",
    "def metropolis_hastings(number_of_steps,w0,b0,model,X,Y,noise):\n",
    "    #initializing\n",
    "    w=w0\n",
    "    b=b0\n",
    "    #looping for number of iteration: here is the MCMC \n",
    "    for step in range(number_of_steps):\n",
    "        # Building new test vector\n",
    "        w_test = w + step_dist(shape=model.dim)\n",
    "        b_test = b + step_dist()\n",
    "        #computing alpha\n",
    "        alpha = model.bayesian_proba(X,w_test,b_test,Y,noise)/model.bayesian_proba(X,w,b,Y,noise)\n",
    "        if alpha >=1:\n",
    "            w = w_test\n",
    "            b = b_test\n",
    "        else:\n",
    "            if  random.random() < alpha:\n",
    "                w = w_test\n",
    "                b = b_test\n",
    "    return w.tolist(),b[0]\n",
    "\n",
    "def run_linear_mcmc():\n",
    "    # number of steps of the MCMC\n",
    "    number_of_steps=2000\n",
    "    # number of sample of the MCMC\n",
    "    number_of_sample=1500\n",
    "    # Size of the dataset\n",
    "    size_batch = 100\n",
    "    # Dimension of the intput X\n",
    "    dim = 2\n",
    "    # Noise variance that we will add to the output\n",
    "    noise = 0.1\n",
    "    linear = linearModel(0,1,dim)\n",
    "    # Building toy dataset\n",
    "    X = np.random.rand(size_batch,dim)*2 -1\n",
    "    #Selecting random values that will have to be found by the linear model\n",
    "    W = np.random.randn(dim)\n",
    "    B = np.random.randn(1)\n",
    "    #computing noisy output that we will try to approximate\n",
    "    Y = np.dot(X,W) + B + np.random.randn(size_batch)*noise\n",
    "    print(\"Parameters to guess:\")\n",
    "    print(\"W:\",W)\n",
    "    print(\"B: \",B)\n",
    "    print(\"_____________________\")\n",
    "    #initialization at 0 for all parameters\n",
    "    w0 = np.zeros(dim)\n",
    "    b0 = np.zeros(1)\n",
    "\n",
    "    # sampling from the posterior\n",
    "    w,b = zip(*[metropolis_hastings(number_of_steps,w0,b0,linear,X,Y,noise) for _ in tqdm(range(number_of_sample))])\n",
    "    unzipped_w = zip(*w)\n",
    "    i=0\n",
    "    #printing average of the posterior\n",
    "    print(\"Average parameters:\")\n",
    "    print(\"W: \",[np.mean(ws) for ws in unzipped_w])\n",
    "    print(\"B: \",np.mean(b))\n",
    "    #plots\n",
    "    for w_single in unzipped_w:\n",
    "        plt.figure()\n",
    "        plt.hist(w_single,bins=40)\n",
    "        plt.title(\"w\"+str(i))\n",
    "        i += 1\n",
    "    plt.figure()\n",
    "    plt.hist(b,bins=40)\n",
    "    plt.title(\"b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to guess:\n",
      "W: [ 0.66472588 -0.06998935]\n",
      "B:  [0.0735572]\n",
      "_____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [13:36<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average parameters:\n",
      "W:  [0.6696426454030919, -0.059652492554884855]\n",
      "B:  0.09262194689561934\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_linear_mcmc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These result are great but this is definitly taking too long I will present here quick solutions for speeding up the sampling the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burn - in, Acceptance rate\n",
    "Between each iterations we reset the value to our $x_0$ however it is not the best strategy: it takes some time for $x$ to find the right set of parameters, what if we were to initialize the new MCMC at the end of the previously chain: the point already belongs to the posterior distribution, we just have to wait some iterations so as to decorrelate $x_0$ and $x_t$.\n",
    "\n",
    "For now we chose abritrarily the step size of our transition however some heuristics exists: the acceptance rate is the average percentage of times that we accept the new value in the Metropolis algorithm. If it is close to 0 it means that $\\alpha$ is always too small: the step brings $x$ to much less probable regions: the step is too big. On the other hand an acceptance rate too high means the step size is too big. Generally we try to have an acceptance rate of 0.4. Some more finely tuned Metropolis algorithms change the step size according to the acceptance rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
